{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQaBBkypRU22rFDc5mErcE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCTWmKnc9X_u","executionInfo":{"status":"ok","timestamp":1746596540710,"user_tz":-330,"elapsed":17562,"user":{"displayName":"Tushar Gosavi","userId":"07226340499732642531"}},"outputId":"c6f573eb-0d6e-438c-c4bb-f3f0e63e6f3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["25000 train sequences\n","25000 test sequences\n","Pad sequences (samples x time)\n","train_data shape: (25000, 256)\n","test_data shape: (25000, 256)\n","Training the model...\n","Epoch 1/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.5484 - loss: 0.6916 - val_accuracy: 0.7109 - val_loss: 0.6837\n","Epoch 2/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6866 - loss: 0.6794 - val_accuracy: 0.6856 - val_loss: 0.6667\n","Epoch 3/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7227 - loss: 0.6571 - val_accuracy: 0.7525 - val_loss: 0.6367\n","Epoch 4/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7659 - loss: 0.6224 - val_accuracy: 0.7788 - val_loss: 0.5949\n","Epoch 5/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7891 - loss: 0.5760 - val_accuracy: 0.7901 - val_loss: 0.5499\n","Epoch 6/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8121 - loss: 0.5232 - val_accuracy: 0.7906 - val_loss: 0.5039\n","Epoch 7/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8313 - loss: 0.4754 - val_accuracy: 0.8245 - val_loss: 0.4601\n","Epoch 8/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8498 - loss: 0.4302 - val_accuracy: 0.8402 - val_loss: 0.4257\n","Epoch 9/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8548 - loss: 0.3997 - val_accuracy: 0.8338 - val_loss: 0.4029\n","Epoch 10/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8674 - loss: 0.3713 - val_accuracy: 0.8536 - val_loss: 0.3757\n","Evaluating the model...\n","Test Loss: 0.3813\n","Test Accuracy: 0.8515\n"]}],"source":["import numpy as np\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n","\n","# 1. Load and Prepare the Dataset\n","\n","max_words = 10000\n","\n","maxlen = 256\n","\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_words)\n","print(len(train_data), 'train sequences')\n","print(len(test_data), 'test sequences')\n","\n","print('Pad sequences (samples x time)')\n","train_data = pad_sequences(train_data, maxlen=maxlen)\n","test_data = pad_sequences(test_data, maxlen=maxlen)\n","print('train_data shape:', train_data.shape)\n","print('test_data shape:', test_data.shape)\n","\n","# Convert labels to float32 (standard for neural network inputs)\n","train_labels = np.asarray(train_labels).astype('float32')\n","test_labels = np.asarray(test_labels).astype('float32')\n","\n","\n","# 2. Build the Model\n","\n","embedding_dim = 16 # Dimension of the word embeddings\n","\n","model = Sequential()\n","# Embedding layer: converts word indices into dense vectors of fixed size\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","# Global Average Pooling: A simpler way to handle variable length sequences\n","# by averaging the embeddings across the sequence dimension.\n","model.add(GlobalAveragePooling1D())\n","# Dense layers for classification\n","model.add(Dense(16, activation='relu'))\n","model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n","\n","# 3. Compile the Model\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy', # Appropriate loss for binary classification\n","              metrics=['accuracy'])\n","\n","# 4. Train the Model\n","\n","# Create a validation set from the training data\n","x_val = train_data[:10000]\n","partial_x_train = train_data[10000:]\n","\n","y_val = train_labels[:10000]\n","partial_y_train = train_labels[10000:]\n","\n","\n","print('Training the model...')\n","history = model.fit(partial_x_train,\n","                    partial_y_train,\n","                    epochs=10, # Reduced epochs as embedding layer learns faster\n","                    batch_size=512,\n","                    validation_data=(x_val, y_val),\n","                    verbose=1) # Show training progress\n","\n","# 5. Evaluate the Model\n","\n","print('Evaluating the model...')\n","loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)\n","\n","print(f'Test Loss: {loss:.4f}')\n","print(f'Test Accuracy: {accuracy:.4f}')\n"]}]}